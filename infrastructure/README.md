# TTB Verifier Infrastructure

This directory contains OpenTofu/Terraform code managed by Terragrunt to provision AWS infrastructure for the TTB Label Verifier API.

## üìñ Complete Deployment Guide

**üëâ For full deployment instructions, see [DEPLOYMENT_GUIDE.md](./DEPLOYMENT_GUIDE.md)**

The deployment guide includes:
- Detailed prerequisite installation steps
- Complete configuration instructions  
- Step-by-step deployment process
- DNS configuration details
- Troubleshooting common issues
- Cost estimates and security considerations

## Quick Overview

This infrastructure is fully configurable and can be deployed to any AWS account and GitHub organization by simply editing `terraform.tfvars`.

## Two-Layer Architecture

The infrastructure is separated into **two independent layers** with separate Terraform state files:

### Foundation Layer (`foundation/`)
**Protected, long-lived resources deployed once:**
- ACM Certificate (HTTPS/TLS)
- GitHub Repository (code + settings)
- S3 Bucket for Ollama models (performance optimization)
- State: `s3://unitedentropy-ttb-tfstate/foundation/terraform.tfstate`

**Key Features:**
- All resources have `prevent_destroy = true` lifecycle protection
- Rarely modified after initial deployment
- Preserves critical resources during application layer updates

### Application Layer (`.`)
**Ephemeral resources that can be destroyed/recreated:**
- EC2 instance (Docker host)
- Application Load Balancer + listeners
- IAM roles and policies
- Security groups
- GitHub Actions secrets
- State: `s3://unitedentropy-ttb-tfstate/infrastructure/terraform.tfstate`

**Key Features:**
- References foundation via Terraform remote state
- Can be destroyed with `terragrunt destroy` (no targeting required)
- Fast disaster recovery (8-12 minutes with S3 model cache)

### Cross-Layer Communication

Application layer accesses foundation resources via remote state data source:
```hcl
data "terraform_remote_state" "foundation" {
  backend = "s3"
  config = {
    bucket = "unitedentropy-ttb-tfstate"
    key    = "foundation/terraform.tfstate"
    region = "us-east-1"
  }
}

# Example usage
locals {
  certificate_arn = data.terraform_remote_state.foundation.outputs.certificate_arn
  s3_bucket_id    = data.terraform_remote_state.foundation.outputs.s3_bucket_id
}
```

## Application Architecture Overview

```
Internet (HTTPS)
    ‚Üì
Application Load Balancer
    ‚Üì (HTTP, port 8000)
EC2 t3.medium (Docker)
    ‚îú‚îÄ ttb-verifier container (FastAPI)
    ‚îî‚îÄ ttb-ollama container (AI OCR)
```

**Key Features:**
- ‚úÖ HTTPS with auto-renewing ACM certificate
- ‚úÖ SSM-based deployment (no SSH keys)
- ‚úÖ OIDC authentication for GitHub Actions (no long-lived credentials)
- ‚úÖ Default VPC (reuses existing subnets, no custom VPC needed)
- ‚úÖ Automated Ollama model pre-loading during first boot

## Prerequisites

### Required Tools

- **OpenTofu:** v1.11.5+ ([Install](https://opentofu.org/docs/intro/install/))
- **Terragrunt:** Latest version ([Install](https://terragrunt.gruntwork.io/docs/getting-started/install/))
- **AWS CLI:** v2+ configured with credentials ([Install](https://aws.amazon.com/cli/))

### AWS Permissions Required

Your AWS IAM user/role needs permissions to create:
- EC2 instances and security groups
- IAM roles, policies, and OIDC providers
- Application Load Balancers and target groups
- ACM certificates
- S3 buckets (for Terragrunt state)
- DynamoDB tables (for state locking)

Recommended: Use `AdministratorAccess` policy for initial setup, then restrict.

### DNS Access

You'll need access to your **DNS provider** to add:
1. CNAME for ACM certificate validation (temporary)
2. CNAME for the application pointing to the ALB (permanent)

## Deployment Order

### First-Time Setup (Both Layers)

**1. Deploy Foundation Layer:**
```bash
cd infrastructure/foundation
terragrunt init
terragrunt plan
terragrunt apply
```

**Time:** ~5 minutes  
**Creates:** ACM certificate, GitHub repository, S3 model bucket

**2. Deploy Application Layer:**
```bash
cd infrastructure  # parent directory
terragrunt init
terragrunt plan
terragrunt apply
```

**Time:** ~8-15 minutes (includes EC2 initialization + model download)  
**Creates:** EC2, ALB, IAM roles, security groups, GitHub secrets

### Day-to-Day Operations

**For most infrastructure changes**, you'll only work in the application layer:

```bash
cd infrastructure
terragrunt plan
terragrunt apply
```

**Foundation layer** is only modified when:
- Changing the domain name (ACM certificate)
- Modifying repository settings
- Updating S3 bucket configuration

### Disaster Recovery

**To test complete infrastructure rebuild:**

```bash
# Destroy application layer (safe - foundation protected)
cd infrastructure
terragrunt destroy  # No targeting required!

# Recreate application layer
terragrunt apply
```

**RTO:** 8-12 minutes (uses S3 model cache)

**Note:** Foundation layer resources have `prevent_destroy = true` and cannot be accidentally destroyed.

## Quick Start (Existing Infrastructure)

If the infrastructure already exists and you just need to make changes:

## Quick Start (Existing Infrastructure)

If the infrastructure already exists and you just need to make changes:

### Step 1: Initialize Terragrunt (Application Layer)

```bash
cd infrastructure  # application layer
terragrunt init
```

**What happens:**
- Connects to existing S3 backend: `unitedentropy-ttb-tfstate`
- Uses state file: `infrastructure/terraform.tfstate`
- Downloads AWS provider plugins
- Generates `backend.tf` and `provider.tf`

**Time:** ~30 seconds

### Step 2: Plan Infrastructure Changes

```bash
terragrunt plan
```

**Review the plan:**
- Shows what will be added, changed, or destroyed
- Includes references to foundation resources via remote state

**Time:** ~1 minute

### Step 3: Apply Infrastructure Changes

```bash
terragrunt apply
```

**What happens:**

1. **Resources Created** (~5 minutes):
   - Security groups
   - IAM roles and policies
   - OIDC provider
   - EC2 instance launched
   - ALB and target group created
   - ACM certificate requested

2. **Interactive Pause** (5-30 minutes):
   - Script outputs DNS validation CNAME record
   - **YOU MUST:** Log into your DNS provider and add the CNAME record
   - Script polls ACM every 30 seconds waiting for validation
   - Continues automatically once certificate is validated

3. **EC2 Initialization** (~15 minutes in background):
   - User data script installs Docker + Docker Compose
   - Pulls Ollama container
   - Downloads llama3.2-vision model (~7.9GB)
   - Creates `/app/` directory and deployment scripts

4. **Completion**:
   - HTTPS listener created (uses validated certificate)
   - EC2 registered with ALB
   - Outputs displayed

**Total Time:** 25-50 minutes (depends on DNS propagation speed)

### Step 4: Configure DNS (Final CNAME)

After `terragrunt apply` completes, get the ALB DNS name:

```bash
terragrunt output alb_dns_name
```

**Add CNAME to your DNS provider:**
- **Hostname:** Your application domain (e.g., `ttb-verifier.yourdomain.com`)
- **Type:** CNAME
- **Value:** `<alb-dns-name from output>` (e.g., `ttb-verifier-alb-123456789.us-east-1.elb.amazonaws.com`)
- **TTL:** 300 (or default)

**Wait 5-10 minutes for DNS propagation.**

### Step 5: Verify Infrastructure

```bash
# Test DNS resolution
nslookup ttb-verifier.unitedentropy.com

# Test ALB (will show error until application deployed)
curl https://ttb-verifier.unitedentropy.com

# Check EC2 via SSM
aws ssm start-session --target $(terragrunt output -raw ec2_instance_id)

# Once in SSM session, verify:
docker ps                  # Should show ollama container
docker-compose -f /app/docker-compose.yml logs
```

### Step 6: Configure GitHub Secrets

Get the outputs:

```bash
terragrunt output ec2_instance_id
terragrunt output github_actions_role_arn
```

**Add to GitHub Repository** (Settings ‚Üí Secrets and variables ‚Üí Actions):

1. **Name:** `AWS_ROLE_TO_ASSUME`  
   **Value:** Output from `github_actions_role_arn`  
   **Example:** `arn:aws:iam::253490750467:role/ttb-github-actions-role`

2. **Name:** `EC2_INSTANCE_ID`  
   **Value:** Output from `ec2_instance_id`  
   **Example:** `i-0abc123def456789`

3. **Name:** `AWS_REGION` (optional, can use workflow env var)  
   **Value:** `us-east-1`

## Resources Created

### Compute
- **EC2 Instance:** t3.medium, Amazon Linux 2023, 30GB gp3
- **IAM Instance Profile:** Attached to EC2 for SSM access

### Networking
- **Application Load Balancer:** Internet-facing, HTTPS + HTTP
- **Target Group:** Routes to EC2 port 8000
- **Security Groups:**
  - ALB: Allow 80/443 from internet
  - EC2: Allow 8000 from ALB only

### Security & Identity
- **IAM Role (EC2):** `ttb-ssm-role` with `AmazonSSMManagedInstanceCore`
- **IAM Role (GitHub Actions):** `ttb-github-actions-role` with SSM SendCommand permissions
- **OIDC Provider:** GitHub Actions authentication (no long-lived credentials)

### SSL/TLS
- **ACM Certificate:** `ttb-verifier.unitedentropy.com` (auto-renewing, DNS validated)

### State Management
- **S3 Bucket:** `unitedentropy-ttb-tfstate` (auto-created by Terragrunt)
- **DynamoDB Table:** `unitedentropy-ttb-tfstate` (state locking)

## Configuration Details

### EC2 Instance

**Specifications:**
- Type: t3.medium (2 vCPU, 4GB RAM)
- AMI: Amazon Linux 2023 (latest)
- Storage: 30GB gp3 (for Docker images + Ollama models)
- Network: Default VPC, public subnet (reuses existing infrastructure)

**Installed Software** (via user_data):
- Docker Engine
- Docker Compose
- Amazon SSM Agent (pre-installed, verified)
- Pre-pulled: ollama/ollama:latest
- Pre-downloaded: llama3.2-vision model (~7.9GB)

**Directory Structure:**
```
/app/
‚îú‚îÄ‚îÄ docker-compose.yml    # Production compose config
‚îî‚îÄ‚îÄ deploy.sh             # Deployment script called by GitHub Actions
```

### Security Groups

**ALB Security Group (`ttb-alb-sg`):**
- Inbound:
  - Port 80 from 0.0.0.0/0 (HTTP, redirects to HTTPS)
  - Port 443 from 0.0.0.0/0 (HTTPS)
- Outbound: All (for health checks to EC2)

**EC2 Security Group (`ttb-ec2-sg`):**
- Inbound:
  - Port 8000 from ALB security group only
  - NO SSH access (SSM only)
- Outbound: All (for Docker pulls, SSM agent communication)

### Load Balancer Configuration

**Listeners:**
- HTTP (80): Redirects to HTTPS (301)
- HTTPS (443): Forwards to target group (port 8000)

**Target Group Health Checks:**
- Path: `/`
- Protocol: HTTP
- Port: 8000
- Interval: 30 seconds
- Timeout: 5 seconds
- Healthy threshold: 2 consecutive successes
- Unhealthy threshold: 3 consecutive failures

**SSL Policy:** `ELBSecurityPolicy-TLS13-1-2-2021-06` (TLS 1.2+)

## Cost Estimate

| Resource | Monthly Cost |
|----------|--------------|
| EC2 t3.medium (730 hours) | $30.37 |
| EBS gp3 (30GB) | $2.40 |
| Application Load Balancer | $16.20 |
| ALB LCU (low traffic) | ~$3.00 |
| Data transfer | ~$2.00 |
| S3 state storage | $0.01 |
| DynamoDB state locking | $0.10 |
| ACM Certificate | **FREE** |
| **Total** | **~$54/month** |

## Updating Infrastructure

### Making Changes

1. Edit `.tf` files as needed
2. Run `terragrunt plan` to preview changes
3. Run `terragrunt apply` to apply changes

### Common Updates

**Scale instance size:**
```hcl
# In instance.tf
instance_type = "t3.large"  # Change from t3.medium
```

**Update security group rules:**
```hcl
# In security_groups.tf
# Add/modify ingress/egress rules
```

**Change health check settings:**
```hcl
# In target_groups.tf
# Modify health_check block
```

## Destroying Infrastructure

‚ö†Ô∏è **WARNING:** This will delete all resources including the EC2 instance.

```bash
terragrunt destroy
```

**What gets deleted:**
- EC2 instance (application lost!)
- Load balancer
- Security groups
- IAM roles and policies
- ACM certificate

**What remains:**
- S3 state bucket (Terragrunt won't delete)
- DynamoDB table (Terragrunt won't delete)
- DNS records (manual removal required)

## Troubleshooting

### Certificate Validation Hangs

**Symptom:** `terragrunt apply` pauses at certificate validation for >30 minutes

**Cause:** DNS record not added or incorrect

**Solution:**
1. Check the output for the exact CNAME record
2. Verify record added correctly in your DNS provider:
   ```bash
   dig _<random>.yourdomain.com CNAME
   ```
3. Wait for DNS propagation (can take up to 30 mins)
4. If timeout occurs, just run `terragrunt apply` again

### EC2 Not Appearing in SSM

**Symptom:** `aws ssm describe-instance-information` doesn't show instance

**Causes:**
- SSM Agent not running
- IAM instance profile not attached
- Network connectivity issues

**Solution:**
```bash
# Check instance status
aws ec2 describe-instances --instance-ids <instance-id>

# Check IAM role attached
aws ec2 describe-iam-instance-profile-associations

# Check user data logs (via EC2 console ‚Üí Instance ‚Üí Actions ‚Üí Monitor ‚Üí Get system log)
```

### Health Checks Failing

**Symptom:** ALB target shows "unhealthy"

**Causes:**
- Docker containers not started
- Port 8000 not accessible
- Application returning non-200 status

**Solution:**
```bash
# SSH via SSM
aws ssm start-session --target <instance-id>

# Check Docker status
docker ps
docker-compose -f /app/docker-compose.yml logs

# Check port listening
curl http://localhost:8000

# Check security group rules
```

### Terraform State Lock

**Symptom:** `Error: Error acquiring the state lock`

**Cause:** Previous `terragrunt apply` was interrupted

**Solution:**
```bash
# Force unlock (use lock ID from error message)
terragrunt force-unlock <lock-id>
```

## File Reference

| File | Purpose |
|------|---------|
| `terragrunt.hcl` | Terragrunt configuration, backend setup |
| `ami.tf` | Data source for Amazon Linux 2023 AMI |
| `instance.tf` | EC2 instance resource |
| `instance_init.sh` | User data script (runs on first boot) |
| `roles.tf` | IAM roles for EC2 and GitHub Actions |
| `oidc.tf` | GitHub OIDC provider |
| `security_groups.tf` | Security groups for ALB and EC2 |
| `load_balancers.tf` | ALB and listeners |
| `target_groups.tf` | ALB target group |
| `certificates.tf` | ACM certificate and validation logic |
| `outputs.tf` | Output values for GitHub configuration |

## Notes

### Default VPC Usage

This infrastructure uses the **default VPC** in us-east-1 with existing subnets. No custom VPC, NAT Gateway, or private subnets are created, saving ~$32/month compared to a fully private architecture.

**Subnet IDs** (hardcoded in `load_balancers.tf`):
- subnet-087c34ce6b854b207 (us-east-1a)
- subnet-067659fddc18e6d08 (us-east-1b)
- subnet-0964715de1feee340 (us-east-1c)
- subnet-0e47260fdbba6e0f6 (us-east-1d)
- subnet-08925a7ee9f27c57b (us-east-1e)
- subnet-0b98b5770d8be8909 (us-east-1f)

These subnets are from the AWS default VPC and are reused to avoid infrastructure overhead. The EC2 instance is in a public subnet but secured via security groups (only ALB can access port 8000).

### Security Posture

**Encrypted in Transit:**
- ‚úÖ HTTPS enforced (HTTP redirects to HTTPS)
- ‚úÖ ACM certificate with TLS 1.2+

**Access Control:**
- ‚úÖ No SSH access (SSM Session Manager only)
- ‚úÖ EC2 port 8000 accessible only from ALB
- ‚úÖ GitHub Actions uses OIDC (no AWS access keys)
- ‚úÖ IAM roles follow least privilege

**Audit & Compliance:**
- ‚úÖ All SSM commands logged to CloudTrail
- ‚úÖ All API requests logged by ALB
- ‚úÖ Infrastructure changes tracked in Terragrunt state

**Known Security Considerations:**
- ‚ö†Ô∏è **Public IP on EC2 Instance:** The EC2 instance currently has a public IP address assigned by the default VPC's auto-assign public IP setting. While the security group restricts access (only ALB can reach port 8000), this is not ideal for a production environment.
  
  **Why it exists:** The default VPC has no NAT Gateway or VPC endpoints. The instance needs internet access to:
  - Pull Docker images from Docker Hub
  - Download Ollama models from S3 (no S3 Gateway Endpoint)
  - Communicate with AWS SSM (no SSM VPC Endpoints)
  
  **Production remediation options:**
  1. **Add VPC Endpoints** (recommended): Deploy S3 Gateway Endpoint + SSM VPC Endpoints + NAT Gateway for Docker Hub access (~$50-60/month)
  2. **NAT Gateway only**: Deploy NAT Gateway and move instance to private subnet (~$35-40/month)
  3. **Custom VPC**: Create private subnet architecture with proper NAT and endpoint design
  
  **Current mitigation:** Security group rules strictly limit access. Only the ALB security group can reach port 8000. No SSH access is configured. The public IP provides no actual inbound access due to security group restrictions.
  
  **Risk assessment:** Acceptable for demo/development environments. Should be remediated before production deployment.

## Next Steps

After infrastructure is deployed:

1. ‚úÖ Configure GitHub repository secrets
2. ‚úÖ Create GitHub Actions workflows (see `../.github/workflows/`)
3. ‚úÖ Push code to trigger first deployment
4. ‚úÖ Test HTTPS endpoint: https://ttb-verifier.unitedentropy.com

See parent repository's CI/CD documentation for GitHub Actions setup.
