version: '3.8'

services:
  # Ollama service for AI OCR (optional, improves accuracy)
  ollama:
    image: ollama/ollama:latest
    container_name: ttb-ollama
    ports:
      - "11435:11434"
    volumes:
      - ollama_models:/root/.ollama
    # Note: GPU support disabled for compatibility
    # To enable GPU, uncomment the deploy section below
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    restart: unless-stopped
    networks:
      - ttb-network

  # Main TTB Label Verifier API
  verifier:
    build:
      context: .
      target: production
    container_name: ttb-verifier
    ports:
      - "8000:8000"
    environment:
      # Ollama configuration
      - OLLAMA_HOST=${OLLAMA_HOST:-http://ollama:11434}
      # Model name - change to use custom models from S3 or Ollama registry
      # Custom models should be uploaded to s3://{bucket}/models/{MODEL_NAME}.tar.gz
      - OLLAMA_MODEL=${OLLAMA_MODEL:-llama3.2-vision}
      
      # App configuration
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - MAX_FILE_SIZE_MB=${MAX_FILE_SIZE_MB:-10}
      - MAX_BATCH_SIZE=${MAX_BATCH_SIZE:-50}
      
      # UI configuration - set your domain for host checking
      # For local development, set to localhost or use .env file
      - DOMAIN_NAME=${DOMAIN_NAME:-localhost}
      # Alternatively, set allowed hosts directly:
      # - ALLOWED_HOSTS=${ALLOWED_HOSTS:-["localhost", "127.0.0.1"]}
      
      # AWS configuration
      - AWS_REGION=${AWS_REGION:-us-east-1}
      
      # Temp directory for file uploads (avoid tmpfs space issues)
      - TMPDIR=/app/tmp
      
      # CORS (allow all for prototype)
      - CORS_ORIGINS=${CORS_ORIGINS:-["*"]}

      # Queue configuration (shared with worker via volume)
      - QUEUE_DB_PATH=/app/tmp/queue.db
      - QUEUE_MAX_ATTEMPTS=${QUEUE_MAX_ATTEMPTS:-3}
    volumes:
      # Shared volume: API writes images + queue.db; worker reads them
      - ttb-tmp:/app/tmp
    depends_on:
      - ollama
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 3s
      retries: 3
      start_period: 10s
    restart: unless-stopped
    networks:
      - ttb-network

  # Queue worker â€” processes one Ollama inference at a time
  worker:
    build:
      context: .
      target: worker
    container_name: ttb-worker
    environment:
      - OLLAMA_HOST=${OLLAMA_HOST:-http://ollama:11434}
      - OLLAMA_MODEL=${OLLAMA_MODEL:-llama3.2-vision}
      # Short per-attempt timeout: retried up to QUEUE_MAX_ATTEMPTS times
      - OLLAMA_TIMEOUT_SECONDS=${WORKER_OLLAMA_TIMEOUT_SECONDS:-12}
      - WORKER_POLL_INTERVAL=${WORKER_POLL_INTERVAL:-2}
      - QUEUE_DB_PATH=/app/tmp/queue.db
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
    volumes:
      # Same shared volume as verifier so worker can read uploaded images
      - ttb-tmp:/app/tmp
    depends_on:
      - ollama
    restart: unless-stopped
    networks:
      - ttb-network

volumes:
  ollama_models:
    driver: local
  # Named volume shared between verifier and worker containers
  ttb-tmp:
    driver: local

networks:
  ttb-network:
    driver: bridge
